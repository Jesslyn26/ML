{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction in NN with pytorch\n",
    "\n",
    "torch.autograd is PyTorchâ€™s automatic differentiation engine that powers neural network training. \n",
    "\n",
    "## Training a NN\n",
    "\n",
    "1. Forward Propagation: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess. \n",
    "\n",
    "Basically training by input -> nodes/weights -> output (guesses) \n",
    "\n",
    "2. Backward Propagation: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent (Your coursera course).\n",
    " \n",
    "Output mistake -> optimize itself by travelling back to the nodes -> inputs (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN sample in pytorch\n",
    "\n",
    "#### Description: pretraining with resnet 18 model from torchvision\n",
    "\n",
    "\n",
    "fun fact: resnet 18 (18 layer) model = CNN model that is apart of ResNet (residual networks) family, introducted in 2015. It is known for its simplicity and effectiveness in image classification tasks\n",
    "\n",
    "Residual learning: \n",
    "* uses residual connections (skip connections) that allow gradients to flow through the network more easily, as NN can be too deep sometimes that it can degrade the performance of the model (bottlenecks)\n",
    "* consist of CNN layers, batch normalization layers, ReLu activation function, and fully connected layers\n",
    "\n",
    "more info: https://towardsdatascience.com/understanding-and-visualizing-resnets-442284831be8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# create a model that takes 1 input of imafe with 3 channels, height & width of 64, \n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "#model has shape of (1, 1000)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conducting foward pass\n",
    "prediction = model(data)\n",
    "\n",
    "# conducting backward pass\n",
    "loss = (prediction - labels).sum() # error = loss\n",
    "# the autograd calculates and stores the gradients for each model parameter in the parameter's .grad attribute\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create an optimizer called SGD with learning rate of 0.01 and momentum of 0.9\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "# SGD = Stochastic Gradient Descent\n",
    "# parameters = learnable weights and biases of the model\n",
    "# learning rate = the size of steps that is taken during gradient descent\n",
    "# momentum = usually paired with SGD to control the momentum of the optimizer. it can help to accelerate the optimizer in the relevant direction. 0.9 momentum is a common choice, indicating that the optimizer should remember 90% of the previous update's momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use step to initiate gradient descent\n",
    "# the optimizer will adjust it by its gradient in .grad\n",
    "optim.step() #gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How AutoGrad help in building NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "# putting requires_grad to true will allow the autograd to track the collected gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a and b = parameteres of the NN\n",
    "# Q is the error\n",
    "\n",
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
