{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py\n",
    "* https://peterbloem.nl/blog/transformers\n",
    "* https://arxiv.org/pdf/1810.04805\n",
    "* https://www.kaggle.com/code/chayan8/sentiment-analysis-using-bert-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing new dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers datasets\n",
    "! pip install pytorch-transformers\n",
    "! pip install pandas seaborn matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n",
      "                                                text labels       id\n",
      "0  My favourite food is anything I didn't have to...   [27]  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...   [27]  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj\n",
      "3                        To make her feel threatened   [14]  ed7ypvh\n",
      "4                             Dirty Southern Wankers    [3]  ed0bdzj\n",
      "Train dataset's shape: (43410, 3)\n",
      "Validation dataset's shape: (5426, 3)\n",
      "Test dataset's shape: (5427, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the GoEmotions dataset\n",
    "datasets = load_dataset(\"go_emotions\")\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(datasets)\n",
    "train_df = pd.DataFrame(datasets['train'])\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"Train dataset's shape:\",datasets['train'].shape)\n",
    "print(\"Validation dataset's shape:\",datasets['validation'].shape)\n",
    "print(\"Test dataset's shape:\",datasets['test'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. GPU detected.\n",
      "Number of GPUs: 1\n",
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Properties of GPU 0: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30)\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL\n",
    "# checking if my GPU is ready to be used for training\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "\n",
    "if gpu_available:\n",
    "    print(\"CUDA is available. GPU detected.\")\n",
    "    # Get the number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs: {num_gpus}\")\n",
    "    # Print details about each GPU\n",
    "    for i in range(num_gpus):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {gpu_name}\")\n",
    "        gpu_properties = torch.cuda.get_device_properties(i)\n",
    "        print(f\"Properties of GPU {i}: {gpu_properties}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 43410\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5426\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "        num_rows: 5427\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(data):\n",
    "    return tokenizer(data['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Verify the columns\n",
    "print(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the labels\n",
    "def binarize_labels(examples):\n",
    "    binarized_labels = torch.zeros((len(examples[\"labels\"]), len(dataset[\"train\"].features[\"labels\"].feature.names)), dtype=torch.float)\n",
    "    for i, labels in enumerate(examples[\"labels\"]):\n",
    "        binarized_labels[i, labels] = 1.0\n",
    "    examples[\"labels\"] = binarized_labels\n",
    "    return examples\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(binarize_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=16, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['validation'], batch_size=16, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_output = self.softmax(linear_output)\n",
    "        return final_output\n",
    "\n",
    "# Instantiate the model\n",
    "model = BertClassifier(num_classes=28)  # GoEmotions has 28 emotion labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing and training BERT\n",
    "### Overview steps:\n",
    "1. Encoding label into hot encoders \n",
    "2. Convert dataframe dictionary into dataframe using pandas\n",
    "3. Import BERT tokenizer to tokenize/encode the text \n",
    "5. Created tensor dataset with input ids (originally text), attention masks (from tokenizer/encoder), and label tensors (originally labels) \n",
    "6. Importing pre-trained pretrained BERT model to train machine\n",
    "7. Creatin optimizer and scheduler for training \n",
    "8. Setting up and checking GPU for training\n",
    "9. Train model and evaluate every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize and Encode\n",
    "#### Description:\n",
    "BERT model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing \n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(datasets['train'])\n",
    "df_validation = pd.DataFrame(datasets['validation'])\n",
    "df_test = pd.DataFrame(datasets['test'])\n",
    "\n",
    "# print(df_train['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Labels:\n",
      " 0        [27]\n",
      "1        [27]\n",
      "2         [2]\n",
      "3        [14]\n",
      "4         [3]\n",
      "         ... \n",
      "43405    [18]\n",
      "43406     [6]\n",
      "43407     [3]\n",
      "43408    [13]\n",
      "43409    [17]\n",
      "Name: labels, Length: 43410, dtype: object\n",
      "\n",
      "Train Labels Tensor:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Validation Labels Tensor:\n",
      " tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "type_of_emotion = datasets['train'].features['labels'].feature.names\n",
    "\n",
    "# Function to encode numeric labels to multi-hot vectors \n",
    "def encode_labels(labels):\n",
    "    encoding = [0] * len(type_of_emotion)\n",
    "    for label in labels:\n",
    "        encoding[label] = 1\n",
    "    return encoding\n",
    "\n",
    "# Apply the encoding labels to the DataFrame\n",
    "df_train['encoded_labels'] = df_train['labels'].apply(encode_labels)\n",
    "df_validation['encoded_labels'] = df_validation['labels'].apply(encode_labels)\n",
    "#df_test['encoded_labels'] = df_test['labels'].apply(encode_labels)\n",
    "\n",
    "# Convert the labels to a list of lists\n",
    "labels_train_list = df_train['encoded_labels'].tolist()\n",
    "labels_validation_list = df_validation['encoded_labels'].tolist()\n",
    "#labels_test_list = df_test['encoded_labels'].tolist()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "labels_train_tensor = torch.tensor(labels_train_list, dtype=torch.float32)\n",
    "labels_validation_tensor = torch.tensor(labels_validation_list, dtype=torch.float32)\n",
    "#labels_test_tensor = torch.tensor(labels_test_list, dtype=torch.float32)\n",
    "\n",
    "# Print the resulting tensor to ensure correctness\n",
    "print(\"\\nTrain Labels:\\n\", df_train['labels'])\n",
    "print(\"\\nTrain Labels Tensor:\\n\", labels_train_tensor)\n",
    "print(\"\\nValidation Labels Tensor:\\n\", labels_validation_tensor)\n",
    "#print(\"\\nTest Labels Tensor:\\n\", labels_test_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessl\\miniconda3\\envs\\torch_gpu_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df_train.text.values,\n",
    "    add_special_tokens=True, # addin [CLS] and [SEP]\n",
    "    return_attention_mask=True, # changr input to 1 with actual words and 0 to none\n",
    "    pad_to_max_length=True, \n",
    "    max_length=\t512,\n",
    "    truncation=True,  # length that is longer than max_length will be trucated\n",
    "    return_tensors='pt' # returning tensor in pytorch form\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df_validation.text.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    pad_to_max_length=True,\n",
    "    max_length=\t512,\n",
    "    truncation=True,  \n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 101, 2026, 8837,  ...,    0,    0,    0],\n",
       "         [ 101, 2085, 2065,  ...,    0,    0,    0],\n",
       "         [ 101, 2339, 1996,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
       "         [ 101, 2062, 2066,  ...,    0,    0,    0],\n",
       "         [ 101, 5959, 1996,  ...,    0,    0,    0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "         [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turning each dataset into tensor dataset to be processed\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, \n",
    "                              attention_masks_train,\n",
    "                              labels_train_tensor)\n",
    "\n",
    "dataset_val = TensorDataset(input_ids_val, \n",
    "                            attention_masks_val,\n",
    "                           labels_validation_tensor)\n",
    "\n",
    "dataset_train.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# passing in bert pretrained classification model to fine tune\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                      'bert-base-uncased', \n",
    "                                      num_labels = len(type_of_emotion),\n",
    "                                      output_attentions = False,\n",
    "                                      output_hidden_states = False\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data loaders\n",
    "# batch size reference https://huggingface.co/docs/transformers/model_doc/bert\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    sampler=RandomSampler(dataset_train),\n",
    "    num_workers=4, # subprocesses to use for data loading\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,  #data loadwe will copy tensor to CUDa pinned memory before return (improve GPU stransfer speed)\n",
    "    prefetch_factor=2 # need google more\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val,\n",
    "    sampler=RandomSampler(dataset_val),\n",
    "    batch_size=32,\n",
    "    num_workers=4, \n",
    "    pin_memory=True, \n",
    "    prefetch_factor=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jessl\\miniconda3\\envs\\torch_gpu_env\\lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# optimizer type: \n",
    "# Stochastic Gradient Descent (SGD): The basic optimization algorithm that updates parameters based on the gradient of the loss function.\n",
    "# Adam: A popular variant of SGD that combines adaptive learning rates with momentum.\n",
    "# AdamW: A variant of Adam that also incorporates weight decay to prevent overfitting.\n",
    "# Adagrad: Adapts the learning rate for each parameter based on the historical gradient information.\n",
    "# RMSprop: Root Mean Square Propagation, similar to Adagrad but with an exponentially decaying average of squared gradients.\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(), # optimizing model\n",
    "    lr = 1e-5 ,\n",
    "    eps = 1e-8, # need a further verification\n",
    ")\n",
    "\n",
    "scaler= GradScaler()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "# Scheduler: a learning rate schediler that adjust learning rate during training to improve performance\n",
    "# type of scheduler:\n",
    "# StepLR: decreases the learning rate by a factor of the fized number of epochs\n",
    "# MultiStepLR: similar to StepLR but allows specifying multicle milestones for decreasing learning rate\n",
    "# ExponentialLR: decays the learning rate expotentially over time\n",
    "# Reduce LROnPlateau: decrease learning rate when specific metric stops improving -\n",
    "# CosineAnnealingLR: gradually decreases the learning rate base on cosine\n",
    "# Linear Warmup: linearly increases the learning rate from zero to the specified value over warmup period -\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps = len(dataloader_train)*epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Usage: 0%\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "\n",
    "def get_gpu_usage():\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], stdout=subprocess.PIPE)\n",
    "        gpu_usage = int(result.stdout.decode('utf-8').strip())\n",
    "        return gpu_usage\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gpu_usage = get_gpu_usage()\n",
    "    if gpu_usage is not None:\n",
    "        print(f\"GPU Usage: {gpu_usage}%\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve GPU usage.\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# seeding / setting seeds for various random number to ensure that the result is the same\n",
    "seed_val = 17\n",
    "random.seed(seed_val) # sets seed for python's build in libraries\n",
    "np.random.seed(seed_val) # set seeds for numpy random generator\n",
    "torch.manual_seed(seed_val) # sets seed for pytorch CPU operations\n",
    "torch.cuda.manual_seed_all(seed_val) # set seeds for CUDA operation\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.cuda()\n",
    "print(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "    model.eval()  # sets model to evaluation mode, no dropout and batch normalization layer\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []  # to store predictions and true labels\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):  # iterate over batches of data in validation loader\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)  # move batch tensors to the appropriate device\n",
    "        \n",
    "        # prepare model inputs tensors \n",
    "        inputs = {'input_ids': batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels': batch[2]}\n",
    "        \n",
    "        # forward pass\n",
    "        # performing forward pass without gradient computation\n",
    "        with torch.no_grad():       \n",
    "            # passing input tensors to the model and obtain the outputs (loss and logits - raw output scores) \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        # calculate raw values \n",
    "        loss = outputs[0] \n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        # processing predictions and true labels\n",
    "        logits = logits.detach().cpu().numpy()  # detach logits from computational graph -> to CPU -> to numpy array\n",
    "        label_ids = inputs['labels'].cpu().numpy()  # same concept\n",
    "        \n",
    "        # Ensure that the shapes are consistent before appending\n",
    "        if logits.shape[0] == label_ids.shape[0]:\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "        else:\n",
    "            print(f\"Shape mismatch: logits {logits.shape}, labels {label_ids.shape}\")\n",
    "    \n",
    "    loss_val_avg = loss_val_total / len(dataloader_val) \n",
    "    \n",
    "    # Debugging: Check the shapes before concatenation\n",
    "    print(f\"Predictions shapes: {[pred.shape for pred in predictions]}\")\n",
    "    print(f\"True values shapes: {[val.shape for val in true_vals]}\")\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    \n",
    "    # Final shape check\n",
    "    print(f\"Final shape of predictions: {predictions.shape}\")\n",
    "    print(f\"Final shape of true values: {true_vals.shape}\")\n",
    "    \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up performance metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    assert preds.shape[0] == labels.shape[0], f\"Shape mismatch: preds {preds.shape}, labels {labels.shape}\"\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.argmax(axis=1).flatten()\n",
    "    \n",
    "    # Check the shapes after flattening\n",
    "    print(f\"Shapes after flattening - preds_flat: {preds_flat.shape}, labels_flat: {labels_flat.shape}\")\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in type_of_emotion.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/amp.html\n",
    "\n",
    "Libraries used for training:\n",
    "* tqdm: adding progress bars to loops so we can see training progress\n",
    "* GradScaler: dynamically adjust gradient scale before backward propagation. Ensuring that there is no overflow or underflow by rescalling them to similar as ROC curve (scale 0 to 1)  (https://youtu.be/IkeEadgSy6w)\n",
    "* Autocast: Automatic Mixed Precision (AMP) feature. Accelerate training by leveraging tensor cores on NVIDA GPUs. used around foward pass and loss calculation\n",
    "\n",
    "P.S. gradient scaler and autocast is just used when we want to utilize GPU for training\n",
    "Underflow:  value too small to represent or compute (close to zero)\n",
    "Overflow: values exceed numerical computations, causing memory leackage. can caused infinity\n",
    "\n",
    "underflow and overflow can cause numerical instabilities due numbers being too small and big, causing inaccurate computational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff7a925db7c4d16b6a8675a7367b7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6a45eb1f174d498b702ef09c1857fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.06657235673199803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fcacecaeef42949391b0b4fcceb662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before F1 computation - Predictions: (5426, 28), True values: (5426, 28)\n",
      "Shapes after flattening - preds_flat: (5426,), labels_flat: (5426,)\n",
      "Validation loss: 0.0874639522503404\n",
      "F1 Score (weighted): 0.5603494697969168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1abb81be07e456888b5b14c98199e1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.06068420308651011\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4654fab7012a41d3b27a7f16c401338b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before F1 computation - Predictions: (5426, 28), True values: (5426, 28)\n",
      "Shapes after flattening - preds_flat: (5426,), labels_flat: (5426,)\n",
      "Validation loss: 0.09000845696119701\n",
      "F1 Score (weighted): 0.5632054252509165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a3e4c8c5a84946adb77902b19d2c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.05560672562981166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be22c152fc2f4731ab5bebb9d1485b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before F1 computation - Predictions: (5426, 28), True values: (5426, 28)\n",
      "Shapes after flattening - preds_flat: (5426,), labels_flat: (5426,)\n",
      "Validation loss: 0.09243338590159136\n",
      "F1 Score (weighted): 0.5604165131958991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0177c6084894433baac7c64a3300913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/2714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Update optimizer\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m     39\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\jessl\\miniconda3\\envs\\torch_gpu_env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:453\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[1;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    451\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 453\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    455\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\jessl\\miniconda3\\envs\\torch_gpu_env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:350\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[1;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[1;32mc:\\Users\\jessl\\miniconda3\\envs\\torch_gpu_env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:350\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    349\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    351\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "epochs_to_save = [4, 6, 8, 10]\n",
    "\n",
    "save_dir = 'Models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader_train, \n",
    "                        desc=f'Epoch {epoch}', \n",
    "                        leave=False, \n",
    "                        disable=False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        with autocast():\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs[0]\n",
    "        \n",
    "        # Scale gradients\n",
    "        scaler.scale(loss).backward()\n",
    "        # Update optimizer\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        loss_train_total += loss.item()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    if epoch in epochs_to_save:\n",
    "        model_save_path = os.path.join(save_dir, f'BERT_ft_Epoch{epoch}.model')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "    \n",
    "    # Check shapes before computing F1 score\n",
    "    print(f\"Shapes before F1 computation - Predictions: {predictions.shape}, True values: {true_vals.shape}\")\n",
    "    \n",
    "\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (weighted): {val_f1}')\n",
    "\n",
    "    # has quite stable and not improving validation loss and f-1 score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
